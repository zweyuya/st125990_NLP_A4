{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd0523d",
   "metadata": {},
   "source": [
    "# Task 1 - Training BERT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cac971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12297b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CV\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc59187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cae1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15f67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0384082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d38f8d",
   "metadata": {},
   "source": [
    "### Dataset: \n",
    "\n",
    "Plaintext Wikipedia (full English) - Kaggle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96b5429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 11578\n",
      "Files that will be loaded: 500\n"
     ]
    }
   ],
   "source": [
    "wiki_path = \"English_Wikipedia/fullEnglish\"  \n",
    "\n",
    "# This finds ALL files inside subfolders\n",
    "files = glob.glob(os.path.join(wiki_path, \"*\", \"*\"))\n",
    "\n",
    "print(\"Total files found:\", len(files))\n",
    "# LIMIT to first 500 files\n",
    "files = files[:500]\n",
    "\n",
    "print(\"Files that will be loaded:\", len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7cac22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files: 500\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "print(\"Loaded files:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d36e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_text = \"\"\n",
    "\n",
    "for text in texts:\n",
    "    text = re.sub(r\"<doc.*?>\", \"\", text)\n",
    "    text = re.sub(r\"</doc>\", \"\", text)\n",
    "    clean_text += text + \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d9e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 100000\n"
     ]
    }
   ],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text = re.sub(r\"\\s+\", \" \", clean_text)\n",
    "\n",
    "sentences = re.split(r\"[.!?]\", clean_text)\n",
    "sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "\n",
    "# Limit for training\n",
    "sentences = sentences[:100000]\n",
    "\n",
    "print(\"Total sentences:\", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af28b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28366\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    word_counter.update(sentence.split())\n",
    "\n",
    "# Special tokens\n",
    "vocab = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    "\n",
    "# Keep words appearing at least 5 times\n",
    "vocab += [word for word, freq in word_counter.items() if freq >= 5]\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdefe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "MAX_LEN = 32\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = sentence.split()[:MAX_LEN-2]\n",
    "    \n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    \n",
    "    ids = [word2idx.get(token, word2idx[\"[UNK]\"]) for token in tokens]\n",
    "    \n",
    "    # Padding\n",
    "    while len(ids) < MAX_LEN:\n",
    "        ids.append(word2idx[\"[PAD]\"])\n",
    "    \n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f748573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 49999\n"
     ]
    }
   ],
   "source": [
    "# Create MLM + NSP Dataset\n",
    "\n",
    "import random\n",
    "\n",
    "def create_dataset(sentences):\n",
    "    dataset = []\n",
    "    \n",
    "    for i in range(len(sentences)-1):\n",
    "        \n",
    "        # 50% correct next sentence\n",
    "        if random.random() > 0.5:\n",
    "            sent_a = sentences[i]\n",
    "            sent_b = sentences[i+1]\n",
    "            nsp_label = 1\n",
    "        else:\n",
    "            sent_a = sentences[i]\n",
    "            sent_b = random.choice(sentences)\n",
    "            nsp_label = 0\n",
    "        \n",
    "        combined = sent_a + \" \" + sent_b\n",
    "        input_ids = tokenize(combined)\n",
    "        \n",
    "        mlm_labels = [-100] * MAX_LEN\n",
    "        \n",
    "        # 15% masking\n",
    "        for j in range(1, MAX_LEN-1):\n",
    "            if input_ids[j] != word2idx[\"[PAD]\"] and random.random() < 0.15:\n",
    "                mlm_labels[j] = input_ids[j]\n",
    "                input_ids[j] = word2idx[\"[MASK]\"]\n",
    "        \n",
    "        dataset.append((input_ids, mlm_labels, nsp_label))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = create_dataset(sentences[:50000])\n",
    "print(\"Training samples:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbcb70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, mlm_labels, nsp_label = self.data[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(mlm_labels, dtype=torch.long),\n",
    "            torch.tensor(nsp_label, dtype=torch.long)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b285d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = BERTDataset(dataset)\n",
    "\n",
    "loader = DataLoader(bert_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5008c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_size=128,\n",
    "        max_len=128,\n",
    "        num_layers=2,\n",
    "        num_heads=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(SimpleBERT, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = nn.Embedding(max_len, hidden_size)\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True   # VERY IMPORTANT\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # MLM head\n",
    "        self.mlm_head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # NSP head\n",
    "        self.nsp_head = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        # Embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # MLM output\n",
    "        mlm_logits = self.mlm_head(x)\n",
    "\n",
    "        # NSP uses [CLS] token (first token)\n",
    "        cls_token = x[:, 0, :]\n",
    "        nsp_logits = self.nsp_head(cls_token)\n",
    "\n",
    "        return mlm_logits, nsp_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98eab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 28366\n",
    "model = SimpleBERT(vocab_size)\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ef6ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e337fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_mlm = nn.CrossEntropyLoss()\n",
    "criterion_nsp = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b0b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b8bf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sentences, word2idx, max_len=32):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sentence_a = self.sentences[idx]\n",
    "        sentence_b = self.sentences[idx + 1]\n",
    "\n",
    "        # 50% chance random sentence for NSP\n",
    "        if random.random() > 0.5:\n",
    "            sentence_b = random.choice(self.sentences)\n",
    "            nsp_label = 0  # Not next\n",
    "        else:\n",
    "            nsp_label = 1  # Is next\n",
    "\n",
    "        tokens = [\"[CLS]\"] + sentence_a.split() + [\"[SEP]\"] + sentence_b.split() + [\"[SEP]\"]\n",
    "\n",
    "        # Convert to ids\n",
    "        input_ids = [self.word2idx.get(w, 0) for w in tokens]\n",
    "\n",
    "        # Padding\n",
    "        if len(input_ids) < self.max_len:\n",
    "            input_ids += [self.word2idx[\"[PAD]\"]] * (self.max_len - len(input_ids))\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        # MLM labels (copy)\n",
    "        mlm_labels = input_ids.clone()\n",
    "\n",
    "        # Mask 15% tokens\n",
    "        for i in range(1, len(input_ids) - 1):\n",
    "            if random.random() < 0.15:\n",
    "                input_ids[i] = self.word2idx[\"[MASK]\"]\n",
    "\n",
    "        return input_ids, mlm_labels, torch.tensor(nsp_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37217009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 8.073345177919531\n",
      "Epoch 2 Loss: 7.620009910732374\n",
      "Epoch 3 Loss: 7.577342767404274\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "nsp_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, mlm_labels, nsp_labels in loader:\n",
    "        \n",
    "        mlm_out, nsp_out = model(input_ids)\n",
    "        \n",
    "        mlm_loss = mlm_loss_fn(\n",
    "            mlm_out.view(-1, vocab_size),\n",
    "            mlm_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        nsp_loss = nsp_loss_fn(nsp_out, nsp_labels)\n",
    "        \n",
    "        loss = mlm_loss + nsp_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ebaa993",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83bf1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab size: 28366\n"
     ]
    }
   ],
   "source": [
    "#verify vocab counts\n",
    "\n",
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "    test_vocab = pickle.load(f)\n",
    "\n",
    "print(\"Saved vocab size:\", len(test_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be2e0f",
   "metadata": {},
   "source": [
    "# TASK 2 - Sentence Embedding with Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0afb5c",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "\n",
    "SLI dataset from The Standford Natural Language Processing Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957887ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66200b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_size=128,\n",
    "        max_len=128,\n",
    "        num_layers=2,\n",
    "        num_heads=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(SimpleBERT, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = nn.Embedding(max_len, hidden_size)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Heads (Task 1)\n",
    "        self.mlm_head = nn.Linear(hidden_size, vocab_size)\n",
    "        self.nsp_head = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, return_hidden=False):\n",
    "\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        # Embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # If Task 2 wants hidden states\n",
    "        if return_hidden:\n",
    "            return x\n",
    "\n",
    "        # Task 1 outputs\n",
    "        mlm_logits = self.mlm_head(x)\n",
    "        cls_token = x[:, 0, :]\n",
    "        nsp_logits = self.nsp_head(cls_token)\n",
    "\n",
    "        return mlm_logits, nsp_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba5cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 28366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleBERT(\n",
       "  (token_embedding): Embedding(28366, 128)\n",
       "  (position_embedding): Embedding(128, 128)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlm_head): Linear(in_features=128, out_features=28366, bias=True)\n",
       "  (nsp_head): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Vocab and Rebuild BERT\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Rebuild BERT with SAME architecture as Task 1\n",
    "bert_model = SimpleBERT(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    max_len=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4\n",
    ").to(device)\n",
    "\n",
    "# Load trained weights\n",
    "bert_model.load_state_dict(torch.load(\"bert_model.pth\", map_location=device))\n",
    "bert_model.train()   # We will fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb0a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify SimpleBERT to return hidden states\n",
    "\n",
    "def forward(self, input_ids, return_hidden=False):\n",
    "\n",
    "    batch_size, seq_len = input_ids.size()\n",
    "\n",
    "    position_ids = torch.arange(seq_len, device=input_ids.device)\n",
    "    position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "    token_embeddings = self.token_embedding(input_ids)\n",
    "    position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "    x = token_embeddings + position_embeddings\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.encoder(x)\n",
    "\n",
    "    if return_hidden:\n",
    "        return x   # Return encoder output only\n",
    "\n",
    "    mlm_logits = self.mlm_head(x)\n",
    "    cls_token = x[:, 0, :]\n",
    "    nsp_logits = self.nsp_head(cls_token)\n",
    "\n",
    "    return mlm_logits, nsp_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f057e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SentenceBERT model (SoftmaxLoss Version)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentenceBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SentenceBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        hidden_size = bert_model.hidden_size\n",
    "        \n",
    "        # (u, v, |u-v|) → 3 * hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size * 3, 3)\n",
    "\n",
    "    def mean_pool(self, token_embeddings):\n",
    "        return torch.mean(token_embeddings, dim=1)\n",
    "\n",
    "    def forward(self, input_ids1, input_ids2):\n",
    "\n",
    "        out1 = self.bert(input_ids1, return_hidden=True)\n",
    "        out2 = self.bert(input_ids2, return_hidden=True)\n",
    "\n",
    "        emb1 = self.mean_pool(out1)\n",
    "        emb2 = self.mean_pool(out2)\n",
    "\n",
    "        diff = torch.abs(emb1 - emb2)\n",
    "\n",
    "        features = torch.cat([emb1, emb2, diff], dim=1)\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40afbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_path = r\"D:\\CV\\snli_1.0\\snli_1.0\\snli_1.0_train.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934a3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_snli_jsonl(path):\n",
    "    data = []\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            \n",
    "            # Remove invalid labels\n",
    "            if item[\"gold_label\"] == \"-\":\n",
    "                continue\n",
    "            \n",
    "            data.append({\n",
    "                \"premise\": item[\"sentence1\"],\n",
    "                \"hypothesis\": item[\"sentence2\"],\n",
    "                \"label\": label_to_id(item[\"gold_label\"])\n",
    "            })\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a267186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string labels into numbers\n",
    "\n",
    "def label_to_id(label):\n",
    "    mapping = {\n",
    "        \"entailment\": 0,\n",
    "        \"contradiction\": 1,\n",
    "        \"neutral\": 2\n",
    "    }\n",
    "    return mapping[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2fb57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 549367\n",
      "{'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "train_data = load_snli_jsonl(snli_train_path)\n",
    "\n",
    "print(\"Training examples:\", len(train_data))\n",
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae9cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4000\n",
      "Dev size: 1000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Reduce to 5000 first\n",
    "train_data = random.sample(train_data, 5000)\n",
    "\n",
    "# Split 80% train / 20% validation\n",
    "split_index = int(0.8 * len(train_data))\n",
    "\n",
    "train_split = train_data[:split_index]\n",
    "dev_split = train_data[split_index:]\n",
    "\n",
    "print(\"Train size:\", len(train_split))\n",
    "print(\"Dev size:\", len(dev_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39f5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    def __init__(self, data, word2idx):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(self, sentence, max_len=128):\n",
    "        tokens = sentence.lower().split()\n",
    "\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx:\n",
    "                ids.append(self.word2idx[token])\n",
    "            else:\n",
    "                ids.append(0)   # unknown words → 0 safely\n",
    "\n",
    "        if len(ids) < max_len:\n",
    "            ids += [0] * (max_len - len(ids))   # padding → 0\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        s1 = self.tokenize(item[\"premise\"])\n",
    "        s2 = self.tokenize(item[\"hypothesis\"])\n",
    "        label = torch.tensor(item[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return s1, s2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c41f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader\n",
    "\n",
    "train_dataset = SNLIDataset(train_split, word2idx)\n",
    "dev_dataset = SNLIDataset(dev_split, word2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bedf6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "\n",
    "def tokenize(self, sentence, max_len=128):\n",
    "    tokens = sentence.lower().split()\n",
    "\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token in self.word2idx:\n",
    "            ids.append(self.word2idx[token])\n",
    "        else:\n",
    "            ids.append(0)  # force unknown words to 0 safely\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [0] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02d0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sbert_model = SentenceBERT(bert_model).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(sbert_model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211492de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 1.2426\n",
      "Epoch 1, Step 100, Loss: 1.0985\n",
      "Epoch 1 Finished, Avg Loss: 1.1021\n",
      "Epoch 2, Step 0, Loss: 1.0668\n",
      "Epoch 2, Step 100, Loss: 1.1094\n",
      "Epoch 2 Finished, Avg Loss: 1.0996\n",
      "Epoch 3, Step 0, Loss: 1.1063\n",
      "Epoch 3, Step 100, Loss: 1.0919\n",
      "Epoch 3 Finished, Avg Loss: 1.0991\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    sbert_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, (s1, s2, labels) in enumerate(train_loader):\n",
    "\n",
    "        s1 = s1.to(device)\n",
    "        s2 = s2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = sbert_model(s1, s2)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Finished, Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4535cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save model\n",
    "torch.save(sbert_model.state_dict(), \"sbert_model.pth\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "becfa98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "\n",
    "print(\"Vocabulary saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f51f1",
   "metadata": {},
   "source": [
    "# Task 3 - Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "206340fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrices\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s1, s2, labels in dataloader:\n",
    "            s1 = s1.to(device)\n",
    "            s2 = s2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(s1, s2)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted'\n",
    "    )\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85acc0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.396\n",
      "Precision: 0.26448491782816724\n",
      "Recall   : 0.396\n",
      "F1-score : 0.3170995508982036\n",
      "Confusion Matrix:\n",
      " [[190   0 139]\n",
      " [184   0 149]\n",
      " [132   0 206]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CV\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, cm = evaluate(sbert_model, dev_loader, device)\n",
    "\n",
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall   :\", recall)\n",
    "print(\"F1-score :\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d3b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Metric     Score\n",
      "0   Accuracy  0.396000\n",
      "1  Precision  0.264485\n",
      "2     Recall  0.396000\n",
      "3   F1-score  0.317100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"],\n",
    "    \"Score\": [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c1ad3",
   "metadata": {},
   "source": [
    "### Limitations and Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767265a",
   "metadata": {},
   "source": [
    "There are several challenges that had been faced while implementing these models. \n",
    "\n",
    "Firstly, there has been limitation in obtaining the two datasets (English Wikipedia and SNLI) directly from URL as most of the servers have been blocked for direct use. To overcome this, various datasets from reliable resources (in this case - Kaggle) are browsed and downloaded manually into the machine and loaded into the notebook. \n",
    "\n",
    "Moreover, it took more than days to train the model without errors as tokenization needs further adjustments in merging the models with the first and second tasks. Another thing that has been learnt from this assignment is that data to be trained should be divided effectively from the original dataset as it makes the training time faster and efficient. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
