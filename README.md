# ğŸ§  Natural Language Inference Web App  
### LSTM + Additive Attention | Flask | PyTorch

A deep learning web application that performs **Natural Language Inference (NLI)**.  
Users enter a **Premise** and a **Hypothesis**, and the model predicts:

- âœ… Entailment  
- âŒ Contradiction  
- âš–ï¸ Neutral  

Built using **PyTorch** and deployed with **Flask**.

---

## ğŸŒŸ Demo Preview

| Input | Output |
|-------|--------|
| Premise: A man is playing a guitar. <br> Hypothesis: A person is making music. | âœ… Entailment |

---

### ğŸ›  Requirements

- Python 3.8+

- Flask

- PyTorch

- NumPy

## ğŸš€ Features

- ğŸ”¤ Custom Tokenizer (max length = 128)
- ğŸ§  BiLSTM Encoder
- ğŸ¯ Additive Attention Mechanism
- ğŸ“Š Softmax Classification
- ğŸŒ Clean Flask Web Interface
- ğŸ›¡ Handles `<pad>` and `<unk>` tokens safely

---

## ğŸ— Model Architecture

```bash
app/
â”‚
â”œâ”€â”€ app.py # Flask backend
â”œâ”€â”€ sbert_model.pth # Trained weights
â”œâ”€â”€ vocab.pkl # Trained weights
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html # Frontend UI

```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
cd YOUR_REPO_NAME
```

### Create Virtual Environment (Recommended)

```bash
python -m venv venv
```

For Windows: 

```bash
venv\Scripts\activate
```

For Mac/Linux:

```bash
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install flask torch numpy
```

### â–¶ï¸ Run the Application

```bash
python app.py
```

Open in Browser:

```bash
http://127.0.0.1:5000/
```

### How to use?

1. Enter the sentences in the text boxes for both Premise and Hypothesis.

2. Then click "Predict" to see whether the result is Entailment, Contradiction, or Neutral. 

3. Below is the preview of the interface with the example inputs.  
![Natural Language Interface Web App](screenshots/app.png)


### ğŸ“ˆ Future Improvements

- Deploy to Render / Railway / AWS

- Add REST API endpoint

- Add confidence score display

- Improve UI styling

### âš ï¸ Limitations

- Limited trained dataset.

- The model relies on a fixed word2idx vocabulary built during training.

- Words not seen during training are mapped to <unk>

- Too many unknown words can reduce prediction accuracy

- No subword tokenization (like BPE or WordPiece)

### ğŸ“ˆ Future Improvements

- Retraining on larger portion of dataset. 

- Deploy to Render / Railway / AWS

- Add REST API endpoint

- Improve UI styling

## Appendix

### Evaluation matrices on validation set after training

![evaluation metrices](screenshots/metrices.png)


